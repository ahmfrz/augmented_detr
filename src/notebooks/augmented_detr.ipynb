{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_ids_for_object_classes(coco: COCO, class_names: List[str], img_cnt_per_obj_type: int) -> List[int]:\n",
    "    img_ids = []\n",
    "    for obj_name in class_names:\n",
    "        # filter images based on category\n",
    "        filtered_cat_ids = coco.getCatIds(catNms=[obj_name])\n",
    "        \n",
    "        # find all image ids of the selected object\n",
    "        ids = coco.getImgIds(catIds=filtered_cat_ids)\n",
    "        \n",
    "        # due to resource constraints, limit number of images per class\n",
    "        if len(ids) > img_cnt_per_obj_type:\n",
    "            ids = np.random.choice(ids, img_cnt_per_obj_type)\n",
    "            img_ids.extend(ids.tolist())\n",
    "        else:\n",
    "            img_ids.extend(ids)\n",
    "\n",
    "    # same objects may appear in multiple images\n",
    "    img_ids = list(set(img_ids))\n",
    "    \n",
    "    return img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "def update_config(key, value):\n",
    "    if check_config_exists():\n",
    "        config = load_config()\n",
    "    else:\n",
    "        config = {}\n",
    "    config[key] = value\n",
    "    with open('./augmented_detr_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "def load_config():\n",
    "    with open('./augmented_detr_config.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def check_config_exists():\n",
    "    return os.path.exists('./augmented_detr_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/coco/'\n",
    "ann_file=f'{data_dir}/annotations/instances_val2017.json'\n",
    "\n",
    "obj_types_to_be_selected = 10\n",
    "img_cnt_for_training = 1000\n",
    "img_cnt_per_obj_type = img_cnt_for_training//obj_types_to_be_selected\n",
    "\n",
    "# Initialize the COCO api\n",
    "coco=COCO(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_config_exists():\n",
    "    # Load the categories in a variable\n",
    "    all_cat_ids = coco.getCatIds()\n",
    "    cats = coco.loadCats(all_cat_ids)\n",
    "\n",
    "    def find_category_with_id(class_id: int, categories: List[Dict[str, str]]):\n",
    "        for i in range(len(categories)):\n",
    "            if categories[i]['id']==class_id:\n",
    "                return categories[i]\n",
    "        return None\n",
    "\n",
    "    selected_objects = []\n",
    "    for i in np.random.randint(0,90,obj_types_to_be_selected):\n",
    "        category = find_category_with_id(i, cats)\n",
    "\n",
    "        # check if it's a valid category and it is not already selected\n",
    "        while category is None or category['name'] in selected_objects:\n",
    "            i = np.random.randint(0,90)\n",
    "            category = find_category_with_id(i, cats)\n",
    "\n",
    "        # remember object name\n",
    "        selected_objects.append(category['name'])\n",
    "        print(f'selected id: {i}, super category: {category[\"supercategory\"]}, name: {category[\"name\"]}')\n",
    "\n",
    "    print(f'Total number of images found: {len(get_img_ids_for_object_classes(coco,selected_objects,img_cnt_per_obj_type))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_config_exists():\n",
    "    # selected coco class names\n",
    "    coco_class_names = ['sports ball', 'pizza', 'toaster', 'frisbee', 'clock', 'broccoli', 'apple', 'bench', 'handbag', 'giraffe']\n",
    "    update_config('coco_selected_object_names', coco_class_names)\n",
    "else:\n",
    "    config = load_config()\n",
    "    coco_class_names = config['coco_selected_object_names']\n",
    "\n",
    "img_ids = get_img_ids_for_object_classes(coco,coco_class_names,img_cnt_per_obj_type)\n",
    "\n",
    "# load and display a random image\n",
    "img_id = img_ids[np.random.randint(0,len(img_ids))]\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "I = io.imread(f'{data_dir}/images/val/{img[\"file_name\"]}')/255.0\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img_id,iscrowd=None)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total images\n",
    "len(img_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 960 images are found for object types 'sports ball', 'pizza', 'toaster', 'frisbee', 'clock', 'broccoli', 'apple', 'bench', 'handbag', 'giraffe' with a maximum of 100 per object type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply segmentation mask to loaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((img['height'],img['width']))\n",
    "for i in range(len(anns)):\n",
    "    mask = np.maximum(coco.annToMask(anns[i]), mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find objects that are in selected class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat_ids = coco.getCatIds()\n",
    "cats = coco.loadCats(all_cat_ids)\n",
    "def get_class_name(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "mask = np.zeros((img['height'],img['width']))\n",
    "for i in range(len(anns)):\n",
    "    className = get_class_name(anns[i]['category_id'], cats)\n",
    "    if className in coco_class_names:\n",
    "        pixel_value = coco_class_names.index(className)+1\n",
    "        mask = np.maximum(coco.annToMask(anns[i])*pixel_value, mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_obj_from_ids(img_ids: List[int], coco: COCO):\n",
    "    # load selected images\n",
    "    images = coco.loadImgs(img_ids)\n",
    "        \n",
    "    # Now, filter out the repeated images\n",
    "    unique_images = []\n",
    "    for i in range(len(images)):\n",
    "        if images[i] not in unique_images:\n",
    "            unique_images.append(images[i])\n",
    "                \n",
    "    random.shuffle(unique_images)\n",
    "    \n",
    "    return unique_images\n",
    "\n",
    "unique_images = load_img_obj_from_ids(img_ids, coco)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset COCO to only selected images to save computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def subset_data(mode: str):\n",
    "    if not os.path.exists(f'{data_dir}/images/{mode}/{mode}_subset'):\n",
    "        os.mkdir(f'{data_dir}/images/{mode}/{mode}_subset')\n",
    "    else:\n",
    "        return # already present\n",
    "    \n",
    "    ann_file=f'{data_dir}/annotations/instances_{mode}2017.json'\n",
    "    coco=COCO(ann_file)\n",
    "    \n",
    "    img_ids = get_img_ids_for_object_classes(coco,coco_class_names,img_cnt_per_obj_type)\n",
    "    update_config(f'{mode}_img_ids', img_ids)\n",
    "    \n",
    "    imgs = load_img_obj_from_ids(img_ids, coco)\n",
    "    for img in imgs:\n",
    "        shutil.copyfile(f'{data_dir}/images/{mode}/{img[\"file_name\"]}', f'{data_dir}/images/{mode}/{mode}_subset/{img[\"file_name\"]}')\n",
    "\n",
    "subset_data('val')\n",
    "subset_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_ann(mode: str):\n",
    "    if not os.path.exists(f'{data_dir}/annotations/{mode}_subset'):\n",
    "        os.mkdir(f'{data_dir}/annotations/{mode}_subset')\n",
    "    else:\n",
    "        # pass\n",
    "        return # already present\n",
    "\n",
    "    ann_file=f'{data_dir}/annotations/instances_{mode}2017.json'\n",
    "    with open(ann_file) as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "    img_ids = load_config()[f'{mode}_img_ids']\n",
    "\n",
    "    selected_objs = []\n",
    "    for img in d['images']:\n",
    "        if img['id'] in img_ids:\n",
    "            selected_objs.append(img)\n",
    "\n",
    "    d['images'] = selected_objs\n",
    "\n",
    "    selected_objs = []\n",
    "    coco=COCO(ann_file)\n",
    "    id2label = {k: v['name'] for k,v in coco.cats.items() if v['name'] in coco_class_names}\n",
    "    for i in img_ids:\n",
    "        for a in coco.loadAnns(coco.getAnnIds(i)):\n",
    "            if a['category_id'] in id2label:\n",
    "                selected_objs.append(a)\n",
    "\n",
    "\n",
    "    # for a in d['annotations']:\n",
    "    #     if a['image_id'] in img_ids:\n",
    "    #         selected_objs.append(a)\n",
    "\n",
    "    d['annotations'] = selected_objs\n",
    "\n",
    "    selected_objs = []\n",
    "    filtered_cat_ids = []\n",
    "    for obj_name in coco_class_names:\n",
    "        filtered_cat_ids.append(coco.getCatIds(catNms=[obj_name])[0])\n",
    "\n",
    "    filtered_cat_ids = list(set(filtered_cat_ids))\n",
    "    for c in d['categories']:\n",
    "        if c['id'] in filtered_cat_ids:\n",
    "            selected_objs.append(c)\n",
    "\n",
    "    d['categories'] = selected_objs\n",
    "    \n",
    "    with open(f'{data_dir}/annotations/{mode}_subset/instances_{mode}2017.json', 'w') as f:\n",
    "        json.dump(d, f)\n",
    "\n",
    "\n",
    "subset_ann('val')\n",
    "subset_ann('train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load random image converted to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load image into memory\n",
    "def get_image(image_obj: Dict, img_folder: str, input_image_size: Tuple[int,int], maintain_aspect_ratio: bool = False):\n",
    "    # Read and normalize an image\n",
    "    img = io.imread(f'{img_folder}/{image_obj[\"file_name\"]}', as_gray=True)/255.0\n",
    "    \n",
    "    if maintain_aspect_ratio:\n",
    "        # maintain aspect ratio\n",
    "        height_percent = (input_image_size[0] / float(image_obj['height']))\n",
    "        width = int((float(image_obj['width']) * float(height_percent)))\n",
    "        input_image_size = (input_image_size[0], width)\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, input_image_size)\n",
    "    \n",
    "    return img\n",
    "\n",
    "plt.imshow(get_image(unique_images[np.random.randint(len(unique_images))], \n",
    "                     f'{data_dir}/images/train', (480,256), True))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator to load images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_mask(image_obj: Dict, classes: List, coco: COCO, cat_ids: List, input_image_size: Tuple):\n",
    "    annIds = coco.getAnnIds(image_obj['id'], catIds=cat_ids, iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    cats = coco.loadCats(cat_ids)\n",
    "    train_mask = np.zeros(input_image_size)\n",
    "    for a in range(len(anns)):\n",
    "        class_name = find_category_with_id(\n",
    "            anns[a]['category_id'], cats)['name']\n",
    "        pixel_value = classes.index(class_name)+1\n",
    "        new_mask = cv2.resize(coco.annToMask(\n",
    "            anns[a])*pixel_value, input_image_size)\n",
    "        train_mask = np.maximum(new_mask, train_mask)\n",
    "\n",
    "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
    "    train_mask = train_mask.reshape(\n",
    "        input_image_size[0], input_image_size[1], 1)\n",
    "    return train_mask\n",
    "\n",
    "\n",
    "def get_binary_mask(image_obj: Dict, coco: COCO, cat_ids: List, input_image_size: Tuple):\n",
    "    annIds = coco.getAnnIds(image_obj['id'], catIds=cat_ids, iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    train_mask = np.zeros(input_image_size)\n",
    "    for a in range(len(anns)):\n",
    "        new_mask = cv2.resize(coco.annToMask(anns[a]), input_image_size)\n",
    "\n",
    "        # Threshold because resizing may cause extraneous values\n",
    "        new_mask[new_mask >= 0.5] = 1\n",
    "        new_mask[new_mask < 0.5] = 0\n",
    "\n",
    "        train_mask = np.maximum(new_mask, train_mask)\n",
    "\n",
    "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
    "    train_mask = train_mask.reshape(\n",
    "        input_image_size[0], input_image_size[1], 1)\n",
    "    return train_mask\n",
    "\n",
    "\n",
    "def dataGeneratorCoco(images: List, classes: List, coco: COCO, img_folder: str,\n",
    "                      input_image_size=(224, 224), batch_size=4, mode='train', mask_type='binary'):\n",
    "\n",
    "    dataset_size = len(images)\n",
    "    cat_ids = coco.getCatIds(catNms=classes)\n",
    "\n",
    "    c = 0\n",
    "    while (True):\n",
    "        img = np.zeros(\n",
    "            (batch_size, input_image_size[0], input_image_size[1], 3)).astype('float')\n",
    "        mask = np.zeros(\n",
    "            (batch_size, input_image_size[0], input_image_size[1], 1)).astype('float')\n",
    "\n",
    "        for i in range(c, c+batch_size):  # initially from 0 to batch_size, when c = 0\n",
    "            imageObj = images[i]\n",
    "\n",
    "            # Get image\n",
    "            train_img = get_image(imageObj, img_folder, input_image_size)\n",
    "\n",
    "            # Create Mask\n",
    "            if mask_type == \"binary\":\n",
    "                train_mask = get_binary_mask(\n",
    "                    imageObj, coco, cat_ids, input_image_size)\n",
    "\n",
    "            elif mask_type == \"normal\":\n",
    "                train_mask = get_normal_mask(\n",
    "                    imageObj, classes, coco, cat_ids, input_image_size)\n",
    "\n",
    "            # Add to respective batch sized arrays\n",
    "            img[i-c] = train_img\n",
    "            mask[i-c] = train_mask\n",
    "\n",
    "        c += batch_size\n",
    "        if (c + batch_size >= dataset_size):\n",
    "            c = 0\n",
    "            random.shuffle(images)\n",
    "\n",
    "        yield img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_image_size = (224,224)\n",
    "mask_type = 'binary'\n",
    "\n",
    "val_gen = dataGeneratorCoco(unique_images, coco_class_names, coco, f'{data_dir}/images/train', \n",
    "                            input_image_size, batch_size, 'train', mask_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to visualize generator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generator(gen):\n",
    "    # Iterate the generator to get image and mask batches\n",
    "    img, mask = next(gen)\n",
    " \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    outerGrid = gridspec.GridSpec(1, 2, wspace=0.1, hspace=0.1)\n",
    "   \n",
    "    for i in range(2):        \n",
    "        innerGrid = gridspec.GridSpecFromSubplotSpec(2, 2, subplot_spec=outerGrid[i], wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for j in range(4):\n",
    "            ax = plt.Subplot(fig, innerGrid[j])\n",
    "            if(i==1):\n",
    "                ax.imshow(img[j]);\n",
    "            else:\n",
    "                ax.imshow(mask[j][:,:,0]);\n",
    "                \n",
    "            ax.axis('off')\n",
    "            fig.add_subplot(ax)\n",
    "    plt.show()\n",
    "\n",
    "visualize_generator(val_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper on keras ImageDataGenerator to generate augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentations_generator(gen, aug_generator_args, seed=None):\n",
    "    # Initialize the image data generator with args provided\n",
    "    image_gen = ImageDataGenerator(**aug_generator_args)\n",
    "    \n",
    "    # Remove the brightness argument for the mask. Spatial arguments similar to image.\n",
    "    augGeneratorArgs_mask = aug_generator_args.copy()\n",
    "    _ = augGeneratorArgs_mask.pop('brightness_range', None)\n",
    "    # Initialize the mask data generator with modified args\n",
    "    mask_gen = ImageDataGenerator(**augGeneratorArgs_mask)\n",
    "    \n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    \n",
    "    for img, mask in gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation of the images \n",
    "        # will end up different from the augmentation of the masks\n",
    "        g_x = image_gen.flow(255*img, \n",
    "                             batch_size = img.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = mask_gen.flow(mask, \n",
    "                             batch_size = mask.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        \n",
    "        img_aug = next(g_x)/255.0\n",
    "        \n",
    "        mask_aug = next(g_y)\n",
    "                   \n",
    "\n",
    "        yield img_aug, mask_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augGeneratorArgs = dict(featurewise_center = False, \n",
    "                        samplewise_center = False,\n",
    "                        rotation_range = 5, \n",
    "                        width_shift_range = 0.01, \n",
    "                        height_shift_range = 0.01, \n",
    "                        brightness_range = (0.8,1.2),\n",
    "                        shear_range = 0.01,\n",
    "                        zoom_range = [1, 1.25],  \n",
    "                        horizontal_flip = True, \n",
    "                        vertical_flip = False,\n",
    "                        fill_mode = 'reflect',\n",
    "                        data_format = 'channels_last')\n",
    "\n",
    "aug_gen = augmentations_generator(val_gen, augGeneratorArgs)\n",
    "visualize_generator(aug_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETR implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/coco/'\n",
    "data_dir=os.path.abspath(data_dir)\n",
    "ann_file=f'{data_dir}/annotations/instances_train2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, feature_extractor):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "        \n",
    "        return pixel_values, target\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrFeatureExtractor\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor()#.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=f'{data_dir}/images/train/', \n",
    "                              ann_file=f'{data_dir}/annotations/instances_train2017.json',\n",
    "                              feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder=f'{data_dir}/images/val/', \n",
    "                              ann_file=f'{data_dir}/annotations/instances_val2017.json',\n",
    "                              feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "# let's pick a random image\n",
    "image_ids = train_dataset.coco.getImgIds()\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "\n",
    "print('Image nÂ°{}'.format(image_id))\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(f'{data_dir}/images/train', image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "  class_idx = annotation['category_id']\n",
    "  if id2label[class_idx] in coco_class_names:\n",
    "    box = annotation['bbox']\n",
    "    x,y,w,h = tuple(box)\n",
    "    draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill='white')\n",
    "\n",
    "image\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in annotations:\n",
    "    print(i['category_id'], i['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k: v['name'] for k,v in cats.items() if v['name']}# in coco_class_names}\n",
    "id2label[12] = 'stop sign'\n",
    "id2label[29] = 'handbag'\n",
    "id2label[68] = 'toilet'\n",
    "id2label[69] = 'toilet'\n",
    "id2label[71] = 'tv'\n",
    "len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler, SubsetRandomSampler\n",
    "\n",
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "class SubsetSampler(Sampler):\n",
    "  def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "  def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "def get_selected_image_indices(dataset, img_ids):\n",
    "      idx = []\n",
    "      for i, v in enumerate(list(sorted(dataset.coco.imgs.keys()))):\n",
    "            if v in img_ids:\n",
    "                  idx.append(i)\n",
    "      return idx\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(get_selected_image_indices(train_dataset, load_config()['train_img_ids']))\n",
    "# val_sampler = SubsetRandomSampler(get_selected_image_indices(val_dataset, load_config()['val_img_ids']))\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, sampler=train_sampler, batch_size=2)\n",
    "# val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, sampler=val_sampler, batch_size=2)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch.keys()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['pixel_values'].shape, batch['pixel_mask'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrConfig, DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         self.model = DetrForObjectDetection(DetrConfig(num_of_channels=1, num_of_queries=10))#.from_pretrained(\"facebook/detr-resnet-50\", \n",
    "                                                           #  num_labels=len(id2label),\n",
    "                                                            # ignore_mismatched_sizes=True)\n",
    "         # # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_steps=1, gradient_clip_val=0.1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/facebookresearch/detr.git\n",
    "%cd detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_coco_api_from_dataset\n",
    "from datasets.coco_eval import CocoEvaluator\n",
    "\n",
    "base_ds = get_coco_api_from_dataset(val_dataset) # this is actually just calling the coco attribute\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection\n",
    "\n",
    "# model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "# let's limit the number of batches for now\n",
    "nbatches = 5\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    \n",
    "    if idx > nbatches:\n",
    "        break\n",
    "    \n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs to COCO api\n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the image_id in target to know which image it is\n",
    "pixel_values, target = val_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass to get class logits and bounding boxes\n",
    "outputs = model(pixel_values=pixel_values, pixel_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{id2label[cl.item()]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, outputs, threshold=0.9):\n",
    "  # keep only predictions with confidence >= threshold\n",
    "  probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "  \n",
    "  # convert predicted boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "  # plot results\n",
    "  plot_results(image, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, outputs, threshold=0.9):\n",
    "  # keep only predictions with confidence >= threshold\n",
    "  probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "  \n",
    "  # convert predicted boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "  # plot results\n",
    "  plot_results(image, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(f'{data_dir}/images/train', image['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = target['image_id'].item()\n",
    "image = val_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(f'{data_dir}/images/val', image['file_name']))\n",
    "\n",
    "visualize_predictions(image, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "315af1ae600f1a42e83c536ab8e452b8550c968c1ce50e15d6036443c0c9ac51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
