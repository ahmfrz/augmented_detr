{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_ids_for_object_classes(coco: COCO, class_names: List[str], img_cnt_per_obj_type: int) -> List[int]:\n",
    "    img_ids = []\n",
    "    for obj_name in class_names:\n",
    "        # filter images based on category\n",
    "        filtered_cat_ids = coco.getCatIds(catNms=[obj_name])\n",
    "        \n",
    "        # find all image ids of the selected object\n",
    "        ids = coco.getImgIds(catIds=filtered_cat_ids)\n",
    "        \n",
    "        # due to resource constraints, limit number of images per class\n",
    "        if len(ids) > img_cnt_per_obj_type:\n",
    "            ids = np.random.choice(ids, img_cnt_per_obj_type)\n",
    "            img_ids.extend(ids.tolist())\n",
    "        else:\n",
    "            img_ids.extend(ids)\n",
    "\n",
    "    # same objects may appear in multiple images\n",
    "    img_ids = list(set(img_ids))\n",
    "    \n",
    "    return img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "def update_config(key, value):\n",
    "    if check_config_exists():\n",
    "        config = load_config()\n",
    "    else:\n",
    "        config = {}\n",
    "    config[key] = value\n",
    "    with open('./augmented_detr_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "def load_config():\n",
    "    with open('./augmented_detr_config.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def check_config_exists():\n",
    "    return os.path.exists('./augmented_detr_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/coco/'\n",
    "ann_file=f'{data_dir}/annotations/instances_train2017.json'\n",
    "\n",
    "obj_types_to_be_selected = 5\n",
    "img_cnt_for_training = 300\n",
    "img_cnt_per_obj_type = img_cnt_for_training//obj_types_to_be_selected\n",
    "\n",
    "# Initialize the COCO api\n",
    "coco=COCO(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_config_exists():\n",
    "    # Load the categories in a variable\n",
    "    all_cat_ids = coco.getCatIds()\n",
    "    cats = coco.loadCats(all_cat_ids)\n",
    "\n",
    "    def find_category_with_id(class_id: int, categories: List[Dict[str, str]]):\n",
    "        for i in range(len(categories)):\n",
    "            if categories[i]['id']==class_id:\n",
    "                return categories[i]\n",
    "        return None\n",
    "\n",
    "    selected_objects = []\n",
    "    for i in np.random.randint(0,90,obj_types_to_be_selected):\n",
    "        category = find_category_with_id(i, cats)\n",
    "\n",
    "        # check if it's a valid category and it is not already selected\n",
    "        while category is None or category['name'] in selected_objects:\n",
    "            i = np.random.randint(0,90)\n",
    "            category = find_category_with_id(i, cats)\n",
    "\n",
    "        # remember object name\n",
    "        selected_objects.append(category['name'])\n",
    "        print(f'selected id: {i}, super category: {category[\"supercategory\"]}, name: {category[\"name\"]}')\n",
    "\n",
    "    print(f'Total number of images found: {len(get_img_ids_for_object_classes(coco,selected_objects,img_cnt_per_obj_type))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_config_exists():\n",
    "    # selected coco class names\n",
    "    coco_class_names = ['sports ball', 'frisbee', 'broccoli', 'bench', 'handbag']\n",
    "    update_config('coco_selected_object_names', coco_class_names)\n",
    "else:\n",
    "    config = load_config()\n",
    "    coco_class_names = config['coco_selected_object_names']\n",
    "\n",
    "img_ids = get_img_ids_for_object_classes(coco,coco_class_names,img_cnt_per_obj_type)\n",
    "\n",
    "# load and display a random image\n",
    "img_id = img_ids[np.random.randint(0,len(img_ids))]\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "I = io.imread(f'{data_dir}/train2017/{img[\"file_name\"]}')/255.0\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img_id,iscrowd=None)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total images\n",
    "len(img_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 960 images are found for object types 'sports ball', 'pizza', 'toaster', 'frisbee', 'clock', 'broccoli', 'apple', 'bench', 'handbag', 'giraffe' with a maximum of 100 per object type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply segmentation mask to loaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((img['height'],img['width']))\n",
    "for i in range(len(anns)):\n",
    "    mask = np.maximum(coco.annToMask(anns[i]), mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find objects that are in selected class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat_ids = coco.getCatIds()\n",
    "cats = coco.loadCats(all_cat_ids)\n",
    "def get_class_name(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "mask = np.zeros((img['height'],img['width']))\n",
    "for i in range(len(anns)):\n",
    "    className = get_class_name(anns[i]['category_id'], cats)\n",
    "    if className in coco_class_names:\n",
    "        pixel_value = coco_class_names.index(className)+1\n",
    "        mask = np.maximum(coco.annToMask(anns[i])*pixel_value, mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_obj_from_ids(img_ids: List[int], coco: COCO):\n",
    "    # load selected images\n",
    "    images = coco.loadImgs(img_ids)\n",
    "        \n",
    "    # Now, filter out the repeated images\n",
    "    unique_images = []\n",
    "    for i in range(len(images)):\n",
    "        if images[i] not in unique_images:\n",
    "            unique_images.append(images[i])\n",
    "                \n",
    "    random.shuffle(unique_images)\n",
    "    \n",
    "    return unique_images\n",
    "\n",
    "unique_images = load_img_obj_from_ids(img_ids, coco)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset COCO to only selected images to save computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def subset_data(mode: str):\n",
    "    if not os.path.exists(f'{data_dir}/{mode}2017'):\n",
    "        os.mkdir(f'{data_dir}/{mode}2017')\n",
    "    else:\n",
    "        return # already present\n",
    "    \n",
    "    ann_file=f'{data_dir}/annotations/instances_{mode}2017.json'\n",
    "    coco=COCO(ann_file)\n",
    "    \n",
    "    img_ids = get_img_ids_for_object_classes(coco,coco_class_names,img_cnt_per_obj_type)\n",
    "    update_config(f'{mode}_img_ids', img_ids)\n",
    "    \n",
    "    imgs = load_img_obj_from_ids(img_ids, coco)\n",
    "    for img in imgs:\n",
    "        shutil.copyfile(f'{data_dir}/images/{mode}/{img[\"file_name\"]}', f'{data_dir}/{mode}2017/{img[\"file_name\"]}')\n",
    "\n",
    "subset_data('val')\n",
    "subset_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_ann(mode: str):\n",
    "    ann_file=f'{data_dir}/annotations/instances_{mode}2017.json'\n",
    "    with open(ann_file) as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "    img_ids = load_config()[f'{mode}_img_ids']\n",
    "\n",
    "    selected_objs = []\n",
    "    for img in d['images']:\n",
    "        if img['id'] in img_ids:\n",
    "            selected_objs.append(img)\n",
    "\n",
    "    d['images'] = selected_objs\n",
    "\n",
    "    selected_objs = []\n",
    "    coco=COCO(ann_file)\n",
    "    id2label = {k: v['name'] for k,v in coco.cats.items() if v['name'] in coco_class_names}\n",
    "    for i in img_ids:\n",
    "        for a in coco.loadAnns(coco.getAnnIds(i)):\n",
    "            if a['category_id'] in id2label:\n",
    "                selected_objs.append(a)\n",
    "\n",
    "\n",
    "    # for a in d['annotations']:\n",
    "    #     if a['image_id'] in img_ids:\n",
    "    #         selected_objs.append(a)\n",
    "\n",
    "    d['annotations'] = selected_objs\n",
    "\n",
    "    selected_objs = []\n",
    "    filtered_cat_ids = []\n",
    "    for obj_name in coco_class_names:\n",
    "        filtered_cat_ids.append(coco.getCatIds(catNms=[obj_name])[0])\n",
    "\n",
    "    filtered_cat_ids = list(set(filtered_cat_ids))\n",
    "    for c in d['categories']:\n",
    "        if c['id'] in filtered_cat_ids:\n",
    "            selected_objs.append(c)\n",
    "\n",
    "    d['categories'] = selected_objs\n",
    "    \n",
    "    with open(f'{data_dir}/annotations/instances_{mode}2017.json', 'w') as f:\n",
    "        json.dump(d, f)\n",
    "\n",
    "\n",
    "# subset_ann('val')\n",
    "# subset_ann('train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load random image converted to grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to load image into memory\n",
    "def get_image(image_obj: Dict, img_folder: str, input_image_size: Tuple[int,int], maintain_aspect_ratio: bool = False):\n",
    "    # Read and normalize an image\n",
    "    img = io.imread(f'{img_folder}/{image_obj[\"file_name\"]}', as_gray=True)/255.0\n",
    "    \n",
    "    if maintain_aspect_ratio:\n",
    "        # maintain aspect ratio\n",
    "        height_percent = (input_image_size[0] / float(image_obj['height']))\n",
    "        width = int((float(image_obj['width']) * float(height_percent)))\n",
    "        input_image_size = (input_image_size[0], width)\n",
    "    \n",
    "    # Resize\n",
    "    img = cv2.resize(img, input_image_size)\n",
    "    \n",
    "    return img\n",
    "\n",
    "plt.imshow(get_image(unique_images[np.random.randint(len(unique_images))], \n",
    "                     f'{data_dir}/images/train', (480,256), True))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data generator to load images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normal_mask(image_obj: Dict, classes: List, coco: COCO, cat_ids: List, input_image_size: Tuple):\n",
    "    annIds = coco.getAnnIds(image_obj['id'], catIds=cat_ids, iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    cats = coco.loadCats(cat_ids)\n",
    "    train_mask = np.zeros(input_image_size)\n",
    "    for a in range(len(anns)):\n",
    "        class_name = find_category_with_id(\n",
    "            anns[a]['category_id'], cats)['name']\n",
    "        pixel_value = classes.index(class_name)+1\n",
    "        new_mask = cv2.resize(coco.annToMask(\n",
    "            anns[a])*pixel_value, input_image_size)\n",
    "        train_mask = np.maximum(new_mask, train_mask)\n",
    "\n",
    "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
    "    train_mask = train_mask.reshape(\n",
    "        input_image_size[0], input_image_size[1], 1)\n",
    "    return train_mask\n",
    "\n",
    "\n",
    "def get_binary_mask(image_obj: Dict, coco: COCO, cat_ids: List, input_image_size: Tuple):\n",
    "    annIds = coco.getAnnIds(image_obj['id'], catIds=cat_ids, iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "    train_mask = np.zeros(input_image_size)\n",
    "    for a in range(len(anns)):\n",
    "        new_mask = cv2.resize(coco.annToMask(anns[a]), input_image_size)\n",
    "\n",
    "        # Threshold because resizing may cause extraneous values\n",
    "        new_mask[new_mask >= 0.5] = 1\n",
    "        new_mask[new_mask < 0.5] = 0\n",
    "\n",
    "        train_mask = np.maximum(new_mask, train_mask)\n",
    "\n",
    "    # Add extra dimension for parity with train_img size [X * X * 3]\n",
    "    train_mask = train_mask.reshape(\n",
    "        input_image_size[0], input_image_size[1], 1)\n",
    "    return train_mask\n",
    "\n",
    "\n",
    "def dataGeneratorCoco(images: List, classes: List, coco: COCO, img_folder: str,\n",
    "                      input_image_size=(224, 224), batch_size=4, mode='train', mask_type='binary'):\n",
    "\n",
    "    dataset_size = len(images)\n",
    "    cat_ids = coco.getCatIds(catNms=classes)\n",
    "\n",
    "    c = 0\n",
    "    while (True):\n",
    "        img = np.zeros(\n",
    "            (batch_size, input_image_size[0], input_image_size[1], 3)).astype('float')\n",
    "        mask = np.zeros(\n",
    "            (batch_size, input_image_size[0], input_image_size[1], 1)).astype('float')\n",
    "\n",
    "        for i in range(c, c+batch_size):  # initially from 0 to batch_size, when c = 0\n",
    "            imageObj = images[i]\n",
    "\n",
    "            # Get image\n",
    "            train_img = get_image(imageObj, img_folder, input_image_size)\n",
    "\n",
    "            # Create Mask\n",
    "            if mask_type == \"binary\":\n",
    "                train_mask = get_binary_mask(\n",
    "                    imageObj, coco, cat_ids, input_image_size)\n",
    "\n",
    "            elif mask_type == \"normal\":\n",
    "                train_mask = get_normal_mask(\n",
    "                    imageObj, classes, coco, cat_ids, input_image_size)\n",
    "\n",
    "            # Add to respective batch sized arrays\n",
    "            img[i-c] = train_img\n",
    "            mask[i-c] = train_mask\n",
    "\n",
    "        c += batch_size\n",
    "        if (c + batch_size >= dataset_size):\n",
    "            c = 0\n",
    "            random.shuffle(images)\n",
    "\n",
    "        yield img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "input_image_size = (224,224)\n",
    "mask_type = 'binary'\n",
    "\n",
    "val_gen = dataGeneratorCoco(unique_images, coco_class_names, coco, f'{data_dir}/images/train', \n",
    "                            input_image_size, batch_size, 'train', mask_type)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to visualize generator output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_generator(gen):\n",
    "    # Iterate the generator to get image and mask batches\n",
    "    img, mask = next(gen)\n",
    " \n",
    "    fig = plt.figure(figsize=(20, 10))\n",
    "    outerGrid = gridspec.GridSpec(1, 2, wspace=0.1, hspace=0.1)\n",
    "   \n",
    "    for i in range(2):        \n",
    "        innerGrid = gridspec.GridSpecFromSubplotSpec(2, 2, subplot_spec=outerGrid[i], wspace=0.05, hspace=0.05)\n",
    "\n",
    "        for j in range(4):\n",
    "            ax = plt.Subplot(fig, innerGrid[j])\n",
    "            if(i==1):\n",
    "                ax.imshow(img[j]);\n",
    "            else:\n",
    "                ax.imshow(mask[j][:,:,0]);\n",
    "                \n",
    "            ax.axis('off')\n",
    "            fig.add_subplot(ax)\n",
    "    plt.show()\n",
    "\n",
    "visualize_generator(val_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrapper on keras ImageDataGenerator to generate augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentations_generator(gen, aug_generator_args, seed=None):\n",
    "    # Initialize the image data generator with args provided\n",
    "    image_gen = ImageDataGenerator(**aug_generator_args)\n",
    "    \n",
    "    # Remove the brightness argument for the mask. Spatial arguments similar to image.\n",
    "    augGeneratorArgs_mask = aug_generator_args.copy()\n",
    "    _ = augGeneratorArgs_mask.pop('brightness_range', None)\n",
    "    # Initialize the mask data generator with modified args\n",
    "    mask_gen = ImageDataGenerator(**augGeneratorArgs_mask)\n",
    "    \n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    \n",
    "    for img, mask in gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation of the images \n",
    "        # will end up different from the augmentation of the masks\n",
    "        g_x = image_gen.flow(255*img, \n",
    "                             batch_size = img.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = mask_gen.flow(mask, \n",
    "                             batch_size = mask.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        \n",
    "        img_aug = next(g_x)/255.0\n",
    "        \n",
    "        mask_aug = next(g_y)\n",
    "                   \n",
    "\n",
    "        yield img_aug, mask_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augGeneratorArgs = dict(featurewise_center = False, \n",
    "                        samplewise_center = False,\n",
    "                        rotation_range = 5, \n",
    "                        width_shift_range = 0.01, \n",
    "                        height_shift_range = 0.01, \n",
    "                        brightness_range = (0.8,1.2),\n",
    "                        shear_range = 0.01,\n",
    "                        zoom_range = [1, 1.25],  \n",
    "                        horizontal_flip = True, \n",
    "                        vertical_flip = False,\n",
    "                        fill_mode = 'reflect',\n",
    "                        data_format = 'channels_last')\n",
    "\n",
    "aug_gen = augmentations_generator(val_gen, augGeneratorArgs)\n",
    "visualize_generator(aug_gen)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config():\n",
    "    epochs: int=200\n",
    "    obj_types_to_be_selected: int=5\n",
    "    lr: float=1e-4\n",
    "    weight_decay: float=1e-4\n",
    "    output_dir: str=\"..\\output\"\n",
    "    backbone: str=\"resnet50\"\n",
    "    batch_size: int=9\n",
    "    resume: str=\"..\\detr-r50_no-class-head.pth\"\n",
    "    has_custom_detection_class: bool=False\n",
    "    transform_type: str = 'None'\n",
    "    coco_path=\"..\\data\\coco\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr.main import main, get_args_parser\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def train_detr(config: Config):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([\n",
    "        '--num_queries', str(config.obj_types_to_be_selected),\n",
    "        '--coco_path', config.coco_path,\n",
    "        '--lr', str(config.lr),\n",
    "        '--weight_decay', str(config.weight_decay),\n",
    "        '--output_dir', config.output_dir,\n",
    "        '--backbone', config.backbone,\n",
    "        '--batch_size', str(config.batch_size),\n",
    "        '--resume', \"..\\detr-r50_no-class-head.pth\",\n",
    "        '--epochs', str(config.epochs),\n",
    "        '--has_custom_detection_class', config.has_custom_detection_class,\n",
    "        '--transform_type', config.transform_type\n",
    "    ])\n",
    "\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get pretrained weights\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "    url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "    map_location='cpu',\n",
    "    check_hash=True)\n",
    "\n",
    "# Remove class weights\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "del checkpoint[\"model\"][\"query_embed.weight\"]\n",
    "\n",
    "# Save\n",
    "torch.save(checkpoint,\n",
    "           'detr-r50_no-class-head.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf detr\n",
    "!git clone https://github.com/ahmfrz/detr.git\n",
    "%cd detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.plot_utils import plot_logs\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "log_directory = [Path('../output/')]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'loss',\n",
    "    'mAP',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields_of_interest = (\n",
    "    'loss_ce',\n",
    "    'loss_bbox',\n",
    "    'loss_giou',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fields_of_interest = (\n",
    "    'class_error',\n",
    "    'cardinality_error_unscaled',\n",
    "    )\n",
    "\n",
    "plot_logs(log_directory,\n",
    "          fields_of_interest)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "model = torch.hub.load('facebookresearch/detr',\n",
    "                       'detr_resnet50',\n",
    "                       pretrained=False,\n",
    "                       num_classes=obj_types_to_be_selected)\n",
    "\n",
    "checkpoint = torch.load('../output/checkpoint.pth',\n",
    "                        map_location='cpu')\n",
    "\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "del checkpoint[\"model\"][\"query_embed.weight\"]\n",
    "\n",
    "model.load_state_dict(checkpoint['model'],\n",
    "                      strict=False)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boilerplate functions to display fine-tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "import torchvision.transforms as T\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "\n",
    "def filter_bboxes_from_outputs(outputs,\n",
    "                               threshold=0.7):\n",
    "\n",
    "  # keep only predictions with confidence above threshold\n",
    "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "\n",
    "  probas_to_keep = probas[keep]\n",
    "\n",
    "  # convert boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], im.size)\n",
    "\n",
    "  return probas_to_keep, bboxes_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_finetuned_results(pil_img, prob=None, boxes=None):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if prob is not None and boxes is not None:\n",
    "      for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     fill=False, color=c, linewidth=3))\n",
    "          cl = p.argmax()\n",
    "          text = f'{coco_class_names[cl]}: {p[cl]:0.2f}'\n",
    "          ax.text(xmin, ymin, text, fontsize=15,\n",
    "                  bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(my_image, my_model):\n",
    "  # mean-std normalize the input image (batch-size: 1)\n",
    "  img = transform(my_image).unsqueeze(0)\n",
    "\n",
    "  # propagate through the model\n",
    "  outputs = my_model(img)\n",
    "\n",
    "  for threshold in [0.5, 0.4]:\n",
    "\n",
    "    probas_to_keep, bboxes_scaled = filter_bboxes_from_outputs(outputs,\n",
    "                                                               threshold=threshold)\n",
    "\n",
    "    plot_finetuned_results(my_image,\n",
    "                           probas_to_keep,\n",
    "                           bboxes_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a training image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "coco_class_names = load_config()['coco_selected_object_names']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data_dir = './data/coco/'\n",
    "train_img_ids = load_config()['train_img_ids']\n",
    "coco = COCO(f'{data_dir}/annotations/instances_train2017.json')\n",
    "img_obj = coco.loadImgs(train_img_ids[:5])[4]\n",
    "img_name = f'{data_dir}/train2017/{img_obj[\"file_name\"]}'\n",
    "im = Image.open(img_name)\n",
    "\n",
    "run_workflow(im,\n",
    "            model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "val_img_ids = load_config()['val_img_ids']\n",
    "coco = COCO(f'{data_dir}/annotations/instances_val2017.json')\n",
    "img_obj = coco.loadImgs(val_img_ids[:5])[2]\n",
    "img_name = f'{data_dir}/val2017/{img_obj[\"file_name\"]}'\n",
    "im = Image.open(img_name)\n",
    "\n",
    "run_workflow(im,\n",
    "            model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.coco import CocoDetection\n",
    "\n",
    "data_dir = '../data/coco/'\n",
    "train_dataset = CocoDetection(f'{data_dir}/train2017', f'{data_dir}/annotations/instances_train2017.json', None, True)\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "data_dir = './data/coco/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/examples/example_bboxes/\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "   \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_img_and_anns(mode: str):\n",
    "    img_ids = load_config()[f'{mode}_img_ids']\n",
    "    coco = COCO(f'{data_dir}/annotations/instances_{mode}2017.json')\n",
    "    img_id = np.random.choice(img_ids)\n",
    "    img_obj = coco.loadImgs([img_id])[0]\n",
    "    img = io.imread(f'{data_dir}/{mode}2017/{img_obj[\"file_name\"]}') #Image.open(f'{data_dir}/{mode}2017/{img_obj[\"file_name\"]}')\n",
    "\n",
    "    annIds = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "\n",
    "    return img, anns, img_obj\n",
    "\n",
    "\n",
    "def get_bboxes_and_cats_from_anns(anns):\n",
    "    return [ann['bbox'] for ann in anns], [ann['category_id'] for ann in anns]\n",
    "\n",
    "\n",
    "img, anns, img_obj = get_img_and_anns('train')\n",
    "bboxes, cats = get_bboxes_and_cats_from_anns(anns)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "coco.showAnns(anns, draw_bbox=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "plt.imshow(color.rgb2gray(img))\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to grayscale has no effect on the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, color.rgb2gray(img).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to grayscale removes the third dimension, reducing computational complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=1.0),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Affine(translate_percent=np.random.random_sample(),p=1.0),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Affine(rotate=np.random.randint(1,359), p=1.0),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(350, 350),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img, anns):\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "    for i in range(len(anns)):\n",
    "        mask = np.maximum(coco.annToMask(anns[i]), mask)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, anns, img_obj = get_img_and_anns('train')\n",
    "bboxes, cats = get_bboxes_and_cats_from_anns(anns)\n",
    "mask = get_mask(img, anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects(img, anns, coco):\n",
    "    outputs = []\n",
    "    for ann in anns:\n",
    "        mask = coco.annToMask(ann)\n",
    "        img_cropped = img * mask[:,:,np.newaxis]\n",
    "        rows, cols = np.where(mask)\n",
    "        top_row, bottom_row = rows.min(), rows.max()\n",
    "        left_col, right_col = cols.min(), cols.max()\n",
    "        \n",
    "        if top_row == bottom_row:\n",
    "            bottom_row += 1\n",
    "        \n",
    "        if left_col == right_col:\n",
    "            right_col += 1\n",
    "        \n",
    "        outputs.append((img_cropped[top_row:bottom_row, left_col:right_col], ann))\n",
    "    return outputs\n",
    "\n",
    "coco = COCO(f'{data_dir}/annotations/instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_copies = 2\n",
    "objects = extract_objects(img, anns, coco)\n",
    "img_obj, ann_obj = objects[0]\n",
    "mask_obj = coco.annToMask(ann_obj)\n",
    "\n",
    "pil_img = Image.fromarray(img)\n",
    "\n",
    "for i in range(num_of_copies):\n",
    "    # check if x,y overlap with other objects\n",
    "    mask =np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "    for ann in anns:\n",
    "        mask = np.maximum(coco.annToMask(ann), mask)\n",
    "        \n",
    "    has_overlap = True\n",
    "    while has_overlap:\n",
    "        # generate random positions\n",
    "        x, y = np.random.randint(\n",
    "            0, img.shape[1]), np.random.randint(0, img.shape[0])\n",
    "\n",
    "        mask_bg_cropped = mask[y:y+img_obj.shape[0], x:x+img_obj.shape[1]]\n",
    "        mask_obj_cropped = cv2.resize(\n",
    "            mask_obj, (mask_bg_cropped.shape[1], mask_bg_cropped.shape[0]))\n",
    "\n",
    "        has_overlap = np.bitwise_and(mask_bg_cropped, mask_obj_cropped).any()\n",
    "        \n",
    "        has_overlap = has_overlap or (\n",
    "            (y+img_obj.shape[0]) > img.shape[0]) or ((x+img_obj.shape[1]) > img.shape[1])\n",
    "    \n",
    "    # blend image\n",
    "    alpha = np.ones(img_obj.shape[:2], dtype=np.float32) * 0.7\n",
    "    alpha = np.dstack((alpha, alpha, alpha))\n",
    "    img_obj_alpha = np.concatenate((img_obj, alpha), axis=2)\n",
    "    \n",
    "    # blend with object\n",
    "    img[y:y+img_obj.shape[0], x:x+img_obj.shape[1],\n",
    "        :] = img[y:y+img_obj.shape[0], x:x+img_obj.shape[1], :] * (1-alpha) + img_obj_alpha[:,:,:3] * alpha\n",
    "    \n",
    "    # img[y:y+img_obj.shape[0], x:x+img_obj.shape[1],:] = img_obj\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomErasing\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inproceedings{zhong2020random,\n",
    "# title={Random Erasing Data Augmentation},\n",
    "# author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},\n",
    "# booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},\n",
    "# year={2020}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomErasing(p=1, value='random'),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "pil_img = Image.fromarray(img)\n",
    "transformed = np.array(transform(pil_img))\n",
    "\n",
    "visualize(transformed, bboxes,\n",
    "          cats, id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.GaussNoise(p=1),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# save cropped objects\n",
    "def crop_and_save_objects():\n",
    "    if not os.path.exists('./data/coco/objects'):\n",
    "        os.mkdir('./data/coco/objects')\n",
    "    else:\n",
    "        shutil.rmtree('./data/coco/objects')\n",
    "\n",
    "    coco = COCO(f'{data_dir}/annotations/instances_train2017.json')\n",
    "    img_ids = load_config()['train_img_ids']\n",
    "    img_objs = coco.loadImgs(img_ids)\n",
    "    for img_obj in img_objs:\n",
    "        img = io.imread(f'{data_dir}/train2017/{img_obj[\"file_name\"]}')\n",
    "        annIds = coco.getAnnIds(imgIds=img_obj['id'], iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "\n",
    "        objects = extract_objects(img, anns, coco)\n",
    "        \n",
    "        for o in objects:\n",
    "            obj, ann = o\n",
    "            cv2.imwrite(\n",
    "                f'./data/coco/objects/{img_obj[\"id\"]}_{id2label[ann[\"category_id\"]]}.jpg', obj)\n",
    "\n",
    "\n",
    "crop_and_save_objects()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with geometric augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets.coco import CocoDetection, ConvertCocoPolysToMask, make_coco_transforms\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torchvision\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoAugmented1(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
    "        super(CocoAugmented1, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(f'@@@ CocoAugmented: Index {idx}')\n",
    "        img, target_org = super(CocoAugmented1, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target_org}\n",
    "        print(f'@@@ CocoAugmented: 2')\n",
    "        img, target = self.prepare(img, target)\n",
    "        bboxes, cats = self.get_bboxes_and_cats_from_anns(target_org)\n",
    "        if self._transforms is not None:\n",
    "            print(f'@@@ CocoAugmented: 3')\n",
    "            transformed = self._transforms(\n",
    "                image=np.array(img), bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "            print(f'@@@ CocoAugmented: 4')\n",
    "            img = transformed['image']\n",
    "            target['boxes'] = torch.as_tensor(transformed['bboxes'], dtype=torch.float32).reshape(-1, 4)\n",
    "            target['labels'] = torch.tensor(transformed['category_ids'], dtype=torch.int64)\n",
    "            w, h = img.shape[1], img.shape[0]\n",
    "            target['size'] = torch.as_tensor([int(h), int(w)])\n",
    "            \n",
    "            normalize = A.Compose([\n",
    "                A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "                ToTensorV2()\n",
    "            ],bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "            img = normalize(\n",
    "                image=transformed['image'], bboxes=transformed['bboxes'], category_ids=transformed['category_ids'])['image']\n",
    "\n",
    "        print(f'@@@ CocoAugmented: 5')\n",
    "        return img, target\n",
    "\n",
    "    @staticmethod\n",
    "    def get_bboxes_and_cats_from_anns(anns):\n",
    "        return [ann['bbox'] for ann in anns], [ann['category_id'] for ann in anns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = A.Compose([\n",
    "    A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "geometric_transforms = A.Compose([\n",
    "    A.HorizontalFlip(p=1.0),\n",
    "    # normalize\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "geometric_transforms(image=np.array(img), bboxes=bboxes, category_ids= cats, coco=coco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoDetection1(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms, return_masks):\n",
    "        super(CocoDetection1, self).__init__(img_folder, ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.prepare = ConvertCocoPolysToMask(return_masks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        print(f'@@@ CocoDetection: Index {idx}')\n",
    "        img, target = super(CocoDetection1, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        img, target = self.prepare(img, target)\n",
    "        if self._transforms is not None:\n",
    "            img, target = self._transforms(img, target)\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = CocoDetection1(\"../data/coco/train2017/\", '../data/coco/annotations/instances_train2017.json', make_coco_transforms('train'), False)\n",
    "t[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr.datasets.coco import get_transform\n",
    "from PIL import Image\n",
    "\n",
    "t = get_transform('copypaste', 'train')\n",
    "t(image=img,bboxes=bboxes, category_ids=cats, anns=anns, coco=coco)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETR implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/coco/'\n",
    "data_dir=os.path.abspath(data_dir)\n",
    "ann_file=f'{data_dir}/annotations/instances_train2017.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, feature_extractor):\n",
    "        super(CocoDetection, self).__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {'image_id': image_id, 'annotations': target}\n",
    "        \n",
    "        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze() # remove batch dimension\n",
    "        target = encoding[\"labels\"][0] # remove batch dimension\n",
    "        \n",
    "        return pixel_values, target\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrFeatureExtractor\n",
    "\n",
    "feature_extractor = DetrFeatureExtractor()#.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "\n",
    "train_dataset = CocoDetection(img_folder=f'{data_dir}/images/train/', \n",
    "                              ann_file=f'{data_dir}/annotations/instances_train2017.json',\n",
    "                              feature_extractor=feature_extractor)\n",
    "val_dataset = CocoDetection(img_folder=f'{data_dir}/images/val/', \n",
    "                              ann_file=f'{data_dir}/annotations/instances_val2017.json',\n",
    "                              feature_extractor=feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# based on https://github.com/woctezuma/finetune-detr/blob/master/finetune_detr.ipynb\n",
    "# let's pick a random image\n",
    "image_ids = train_dataset.coco.getImgIds()\n",
    "image_id = image_ids[np.random.randint(0, len(image_ids))]\n",
    "\n",
    "print('Image n{}'.format(image_id))\n",
    "image = train_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(f'{data_dir}/images/train', image['file_name']))\n",
    "\n",
    "annotations = train_dataset.coco.imgToAnns[image_id]\n",
    "draw = ImageDraw.Draw(image, \"RGBA\")\n",
    "\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "\n",
    "for annotation in annotations:\n",
    "  class_idx = annotation['category_id']\n",
    "  if id2label[class_idx] in coco_class_names:\n",
    "    box = annotation['bbox']\n",
    "    x,y,w,h = tuple(box)\n",
    "    draw.rectangle((x,y,x+w,y+h), outline='red', width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill='white')\n",
    "\n",
    "image\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in annotations:\n",
    "    print(i['category_id'], i['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {k: v['name'] for k,v in cats.items() if v['name']}# in coco_class_names}\n",
    "# id2label[12] = 'stop sign'\n",
    "# id2label[29] = 'handbag'\n",
    "# id2label[68] = 'toilet'\n",
    "# id2label[69] = 'toilet'\n",
    "# id2label[71] = 'tv'\n",
    "len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler, SubsetRandomSampler\n",
    "\n",
    "def collate_fn(batch):\n",
    "  pixel_values = [item[0] for item in batch]\n",
    "  encoding = feature_extractor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "  labels = [item[1] for item in batch]\n",
    "  batch = {}\n",
    "  batch['pixel_values'] = encoding['pixel_values']\n",
    "  batch['pixel_mask'] = encoding['pixel_mask']\n",
    "  batch['labels'] = labels\n",
    "  return batch\n",
    "\n",
    "class SubsetSampler(Sampler):\n",
    "  def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "  def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "\n",
    "  def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "def get_selected_image_indices(dataset, img_ids):\n",
    "      idx = []\n",
    "      for i, v in enumerate(list(sorted(dataset.coco.imgs.keys()))):\n",
    "            if v in img_ids:\n",
    "                  idx.append(i)\n",
    "      return idx\n",
    "\n",
    "# train_sampler = SubsetRandomSampler(get_selected_image_indices(train_dataset, load_config()['train_img_ids']))\n",
    "# val_sampler = SubsetRandomSampler(get_selected_image_indices(val_dataset, load_config()['val_img_ids']))\n",
    "\n",
    "# train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, sampler=train_sampler, batch_size=2)\n",
    "# val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, sampler=val_sampler, batch_size=2)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=collate_fn, batch_size=2)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=collate_fn, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch = next(iter(train_dataloader))\n",
    "batch.keys()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['pixel_values'].shape, batch['pixel_mask'].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from transformers import DetrConfig, DetrForObjectDetection\n",
    "import torch\n",
    "\n",
    "class Detr(pl.LightningModule):\n",
    "\n",
    "     def __init__(self, lr, lr_backbone, weight_decay):\n",
    "         super().__init__()\n",
    "         # replace COCO classification head with custom head\n",
    "         self.model = DetrForObjectDetection(DetrConfig(num_of_channels=1, num_of_queries=10))#.from_pretrained(\"facebook/detr-resnet-50\", \n",
    "                                                           #  num_labels=len(id2label),\n",
    "                                                            # ignore_mismatched_sizes=True)\n",
    "         # # see https://github.com/PyTorchLightning/pytorch-lightning/pull/1896\n",
    "         self.lr = lr\n",
    "         self.lr_backbone = lr_backbone\n",
    "         self.weight_decay = weight_decay\n",
    "\n",
    "     def forward(self, pixel_values, pixel_mask):\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "       return outputs\n",
    "     \n",
    "     def common_step(self, batch, batch_idx):\n",
    "       pixel_values = batch[\"pixel_values\"]\n",
    "       pixel_mask = batch[\"pixel_mask\"]\n",
    "       labels = [{k: v.to(self.device) for k, v in t.items()} for t in batch[\"labels\"]]\n",
    "\n",
    "       outputs = self.model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n",
    "\n",
    "       loss = outputs.loss\n",
    "       loss_dict = outputs.loss_dict\n",
    "\n",
    "       return loss, loss_dict\n",
    "\n",
    "     def training_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        # logs metrics for each training_step,\n",
    "        # and the average across the epoch\n",
    "        self.log(\"training_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"train_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def validation_step(self, batch, batch_idx):\n",
    "        loss, loss_dict = self.common_step(batch, batch_idx)     \n",
    "        self.log(\"validation_loss\", loss)\n",
    "        for k,v in loss_dict.items():\n",
    "          self.log(\"validation_\" + k, v.item())\n",
    "\n",
    "        return loss\n",
    "\n",
    "     def configure_optimizers(self):\n",
    "        param_dicts = [\n",
    "              {\"params\": [p for n, p in self.named_parameters() if \"backbone\" not in n and p.requires_grad]},\n",
    "              {\n",
    "                  \"params\": [p for n, p in self.named_parameters() if \"backbone\" in n and p.requires_grad],\n",
    "                  \"lr\": self.lr_backbone,\n",
    "              },\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(param_dicts, lr=self.lr,\n",
    "                                  weight_decay=self.weight_decay)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "     def train_dataloader(self):\n",
    "        return train_dataloader\n",
    "\n",
    "     def val_dataloader(self):\n",
    "        return val_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard.\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Detr(lr=1e-4, lr_backbone=1e-5, weight_decay=1e-4)\n",
    "\n",
    "outputs = model(pixel_values=batch['pixel_values'], pixel_mask=batch['pixel_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "trainer = Trainer(max_steps=1, gradient_clip_val=0.1)\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/facebookresearch/detr.git\n",
    "%cd detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import get_coco_api_from_dataset\n",
    "from datasets.coco_eval import CocoEvaluator\n",
    "\n",
    "base_ds = get_coco_api_from_dataset(val_dataset) # this is actually just calling the coco attribute\n",
    "iou_types = ['bbox']\n",
    "coco_evaluator = CocoEvaluator(base_ds, iou_types) # initialize evaluator with ground truths\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DetrForObjectDetection\n",
    "\n",
    "# model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.coco_eval import CocoEvaluator\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "print(\"Running evaluation...\")\n",
    "\n",
    "# let's limit the number of batches for now\n",
    "nbatches = 5\n",
    "for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "    \n",
    "    if idx > nbatches:\n",
    "        break\n",
    "    \n",
    "    # get the inputs\n",
    "    pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    pixel_mask = batch[\"pixel_mask\"].to(device)\n",
    "    labels = [{k: v.to(device) for k, v in t.items()} for t in batch[\"labels\"]] # these are in DETR format, resized + normalized\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "    orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "    results = feature_extractor.post_process(outputs, orig_target_sizes) # convert outputs to COCO api\n",
    "    res = {target['image_id'].item(): output for target, output in zip(labels, results)}\n",
    "    coco_evaluator.update(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_evaluator.synchronize_between_processes()\n",
    "coco_evaluator.accumulate()\n",
    "coco_evaluator.summarize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use the image_id in target to know which image it is\n",
    "pixel_values, target = val_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = pixel_values.unsqueeze(0).to(device)\n",
    "print(pixel_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass to get class logits and bounding boxes\n",
    "outputs = model(pixel_values=pixel_values, pixel_mask=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# colors for visualization\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def plot_results(pil_img, prob, boxes):\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                   fill=False, color=c, linewidth=3))\n",
    "        cl = p.argmax()\n",
    "        text = f'{id2label[cl.item()]}: {p[cl]:0.2f}'\n",
    "        ax.text(xmin, ymin, text, fontsize=15,\n",
    "                bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, outputs, threshold=0.9):\n",
    "  # keep only predictions with confidence >= threshold\n",
    "  probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "  \n",
    "  # convert predicted boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "  # plot results\n",
    "  plot_results(image, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, outputs, threshold=0.9):\n",
    "  # keep only predictions with confidence >= threshold\n",
    "  probas = outputs.logits.softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "  \n",
    "  # convert predicted boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs.pred_boxes[0, keep].cpu(), image.size)\n",
    "\n",
    "  # plot results\n",
    "  plot_results(image, probas[keep], bboxes_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(f'{data_dir}/images/train', image['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_id = target['image_id'].item()\n",
    "image = val_dataset.coco.loadImgs(image_id)[0]\n",
    "image = Image.open(os.path.join(f'{data_dir}/images/val', image['file_name']))\n",
    "\n",
    "visualize_predictions(image, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f143ba3d2ff9ccf049485e31b2488cd089888a1e124f6ea17afce21c13a32fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
