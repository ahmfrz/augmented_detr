{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import os\n",
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "import skimage.io as io\n",
    "import cv2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_ids_for_object_classes(coco: COCO, class_names: List[str], img_cnt_per_obj_type: int) -> List[int]:\n",
    "    img_ids = []\n",
    "    for obj_name in class_names:\n",
    "        # filter images based on category\n",
    "        filtered_cat_ids = coco.getCatIds(catNms=[obj_name])\n",
    "        \n",
    "        # find all image ids of the selected object\n",
    "        ids = coco.getImgIds(catIds=filtered_cat_ids)\n",
    "        \n",
    "        # due to resource constraints, limit number of images per class\n",
    "        if len(ids) > img_cnt_per_obj_type:\n",
    "            ids = np.random.choice(ids, img_cnt_per_obj_type)\n",
    "            img_ids.extend(ids.tolist())\n",
    "        else:\n",
    "            img_ids.extend(ids)\n",
    "\n",
    "    # same objects may appear in multiple images\n",
    "    img_ids = list(set(img_ids))\n",
    "    \n",
    "    return img_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config\n",
    "def update_config(key, value):\n",
    "    if check_config_exists():\n",
    "        config = load_config()\n",
    "    else:\n",
    "        config = {}\n",
    "    config[key] = value\n",
    "    with open('./augmented_detr_config.json', 'w') as f:\n",
    "        json.dump(config, f, indent=4)\n",
    "\n",
    "def load_config():\n",
    "    with open('./augmented_detr_config.json') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def check_config_exists():\n",
    "    return os.path.exists('./augmented_detr_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir='./data/coco/'\n",
    "ann_file=f'{data_dir}/annotations/instances_train2017.json'\n",
    "\n",
    "obj_types_to_be_selected = 5\n",
    "img_cnt_for_training = 500\n",
    "img_cnt_per_obj_type = img_cnt_for_training//obj_types_to_be_selected\n",
    "\n",
    "# Initialize the COCO api\n",
    "coco=COCO(ann_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_config_exists():\n",
    "    # Load the categories in a variable\n",
    "    all_cat_ids = coco.getCatIds()\n",
    "    cats = coco.loadCats(all_cat_ids)\n",
    "\n",
    "    def find_category_with_id(class_id: int, categories: List[Dict[str, str]]):\n",
    "        for i in range(len(categories)):\n",
    "            if categories[i]['id']==class_id:\n",
    "                return categories[i]\n",
    "        return None\n",
    "\n",
    "    selected_objects = []\n",
    "    for i in np.random.randint(0,90,obj_types_to_be_selected):\n",
    "        category = find_category_with_id(i, cats)\n",
    "\n",
    "        # check if it's a valid category and it is not already selected\n",
    "        while category is None or category['name'] in selected_objects:\n",
    "            i = np.random.randint(0,90)\n",
    "            category = find_category_with_id(i, cats)\n",
    "\n",
    "        # remember object name\n",
    "        selected_objects.append(category['name'])\n",
    "        print(f'selected id: {i}, super category: {category[\"supercategory\"]}, name: {category[\"name\"]}')\n",
    "\n",
    "    print(f'Total number of images found: {len(get_img_ids_for_object_classes(coco,selected_objects,img_cnt_per_obj_type))}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not check_config_exists():\n",
    "    # selected coco class names\n",
    "    coco_class_names = ['sports ball', 'frisbee', 'broccoli', 'bench', 'handbag']\n",
    "    update_config('coco_selected_object_names', coco_class_names)\n",
    "else:\n",
    "    config = load_config()\n",
    "    coco_class_names = config['coco_selected_object_names']\n",
    "\n",
    "img_ids = get_img_ids_for_object_classes(coco,coco_class_names,img_cnt_per_obj_type)\n",
    "\n",
    "# load and display a random image\n",
    "img_id = img_ids[0]\n",
    "img = coco.loadImgs(img_id)[0]\n",
    "I = io.imread(f'{data_dir}/train2017/{img[\"file_name\"]}')/255.0\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(I)\n",
    "\n",
    "annIds = coco.getAnnIds(imgIds=img_id,iscrowd=None)\n",
    "anns = coco.loadAnns(annIds)\n",
    "coco.showAnns(anns)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check total images\n",
    "len(img_ids)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total 960 images are found for object types 'sports ball', 'pizza', 'toaster', 'frisbee', 'clock', 'broccoli', 'apple', 'bench', 'handbag', 'giraffe' with a maximum of 100 per object type"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply segmentation mask to loaded image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros((img['height'],img['width']))\n",
    "for i in range(len(anns)):\n",
    "    mask = np.maximum(coco.annToMask(anns[i]), mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find objects that are in selected class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cat_ids = coco.getCatIds()\n",
    "cats = coco.loadCats(all_cat_ids)\n",
    "def get_class_name(classID, cats):\n",
    "    for i in range(len(cats)):\n",
    "        if cats[i]['id']==classID:\n",
    "            return cats[i]['name']\n",
    "    return \"None\"\n",
    "\n",
    "mask = np.zeros((img['height'],img['width']))\n",
    "for i in range(len(anns)):\n",
    "    className = get_class_name(anns[i]['category_id'], cats)\n",
    "    if className in coco_class_names:\n",
    "        pixel_value = coco_class_names.index(className)+1\n",
    "        mask = np.maximum(coco.annToMask(anns[i])*pixel_value, mask)\n",
    "plt.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load selected images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_obj_from_ids(img_ids: List[int], coco: COCO):\n",
    "    # load selected images\n",
    "    images = coco.loadImgs(img_ids)\n",
    "        \n",
    "    # Now, filter out the repeated images\n",
    "    unique_images = []\n",
    "    for i in range(len(images)):\n",
    "        if images[i] not in unique_images:\n",
    "            unique_images.append(images[i])\n",
    "                \n",
    "    random.shuffle(unique_images)\n",
    "    \n",
    "    return unique_images\n",
    "\n",
    "unique_images = load_img_obj_from_ids(img_ids, coco)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset COCO to only selected images to save computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def subset_data(mode: str):\n",
    "    if not os.path.exists(f'{data_dir}/{mode}2017'):\n",
    "        os.mkdir(f'{data_dir}/{mode}2017')\n",
    "    else:\n",
    "        return # already present\n",
    "    \n",
    "    ann_file=f'{data_dir}/annotations/instances_{mode}2017.json'\n",
    "    coco=COCO(ann_file)\n",
    "    \n",
    "    img_ids = get_img_ids_for_object_classes(coco,coco_class_names,img_cnt_per_obj_type)\n",
    "    update_config(f'{mode}_img_ids', img_ids)\n",
    "    \n",
    "    imgs = load_img_obj_from_ids(img_ids, coco)\n",
    "    for img in imgs:\n",
    "        shutil.copyfile(f'{data_dir}/images/{mode}/{img[\"file_name\"]}', f'{data_dir}/{mode}2017/{img[\"file_name\"]}')\n",
    "\n",
    "subset_data('val')\n",
    "subset_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_ann(mode: str):\n",
    "    ann_file=f'{data_dir}/annotations/instances_{mode}2017.json'\n",
    "    with open(ann_file) as f:\n",
    "        d = json.load(f)\n",
    "        \n",
    "    img_ids = load_config()[f'{mode}_img_ids']\n",
    "\n",
    "    selected_objs = []\n",
    "    for img in d['images']:\n",
    "        if img['id'] in img_ids:\n",
    "            selected_objs.append(img)\n",
    "\n",
    "    d['images'] = selected_objs\n",
    "\n",
    "    selected_objs = []\n",
    "    coco=COCO(ann_file)\n",
    "    id2label = {k: v['name'] for k,v in coco.cats.items() if v['name'] in coco_class_names}\n",
    "    for i in img_ids:\n",
    "        for a in coco.loadAnns(coco.getAnnIds(i)):\n",
    "            if a['category_id'] in id2label:\n",
    "                selected_objs.append(a)\n",
    "\n",
    "\n",
    "    # for a in d['annotations']:\n",
    "    #     if a['image_id'] in img_ids:\n",
    "    #         selected_objs.append(a)\n",
    "\n",
    "    d['annotations'] = selected_objs\n",
    "\n",
    "    selected_objs = []\n",
    "    filtered_cat_ids = []\n",
    "    for obj_name in coco_class_names:\n",
    "        filtered_cat_ids.append(coco.getCatIds(catNms=[obj_name])[0])\n",
    "\n",
    "    filtered_cat_ids = list(set(filtered_cat_ids))\n",
    "    for c in d['categories']:\n",
    "        if c['id'] in filtered_cat_ids:\n",
    "            selected_objs.append(c)\n",
    "\n",
    "    d['categories'] = selected_objs\n",
    "    \n",
    "    with open(f'{data_dir}/annotations/instances_{mode}2017.json', 'w') as f:\n",
    "        json.dump(d, f)\n",
    "\n",
    "\n",
    "# subset_ann('val')\n",
    "# subset_ann('train')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.coco import CocoDetection\n",
    "\n",
    "data_dir = '../data/coco/'\n",
    "train_dataset = CocoDetection(f'{data_dir}/train2017', f'{data_dir}/annotations/instances_train2017.json', None, True)\n",
    "cats = train_dataset.coco.cats\n",
    "id2label = {k: v['name'] for k,v in cats.items()}\n",
    "id2label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "data_dir = './data/coco/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://albumentations.ai/docs/examples/example_bboxes/\n",
    "\n",
    "BOX_COLOR = (255, 0, 0) # Red\n",
    "TEXT_COLOR = (255, 255, 255) # White\n",
    "\n",
    "\n",
    "def visualize_bbox(img, bbox, class_name, color=BOX_COLOR, thickness=2):\n",
    "    \"\"\"Visualizes a single bounding box on the image\"\"\"\n",
    "    x_min, y_min, w, h = bbox\n",
    "    x_min, x_max, y_min, y_max = int(x_min), int(x_min + w), int(y_min), int(y_min + h)\n",
    "   \n",
    "    cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color=color, thickness=thickness)\n",
    "    \n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)    \n",
    "    cv2.rectangle(img, (x_min, y_min - int(1.3 * text_height)), (x_min + text_width, y_min), BOX_COLOR, -1)\n",
    "    cv2.putText(\n",
    "        img,\n",
    "        text=class_name,\n",
    "        org=(x_min, y_min - int(0.3 * text_height)),\n",
    "        fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        fontScale=0.35, \n",
    "        color=TEXT_COLOR, \n",
    "        lineType=cv2.LINE_AA,\n",
    "    )\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize(image, bboxes, category_ids, category_id_to_name):\n",
    "    img = image.copy()\n",
    "    for bbox, category_id in zip(bboxes, category_ids):\n",
    "        class_name = category_id_to_name[category_id]\n",
    "        img = visualize_bbox(img, bbox, class_name)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Color space"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def get_img_and_anns(mode: str):\n",
    "    img_ids = load_config()[f'{mode}_img_ids']\n",
    "    coco = COCO(f'{data_dir}/annotations/instances_{mode}2017.json')\n",
    "    img_id = np.random.choice(img_ids)\n",
    "    img_obj = coco.loadImgs([img_id])[0]\n",
    "    img = io.imread(f'{data_dir}/{mode}2017/{img_obj[\"file_name\"]}') #Image.open(f'{data_dir}/{mode}2017/{img_obj[\"file_name\"]}')\n",
    "\n",
    "    annIds = coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "    anns = coco.loadAnns(annIds)\n",
    "\n",
    "    return img, anns, img_obj\n",
    "\n",
    "\n",
    "def get_bboxes_and_cats_from_anns(anns):\n",
    "    return [ann['bbox'] for ann in anns], [ann['category_id'] for ann in anns]\n",
    "\n",
    "\n",
    "img, anns, img_obj = get_img_and_anns('train')\n",
    "bboxes, cats = get_bboxes_and_cats_from_anns(anns)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "plt.imshow(img)\n",
    "coco.showAnns(anns, draw_bbox=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.axis('off')\n",
    "plt.imshow(color.rgb2gray(img))\n",
    "coco.showAnns(anns, draw_bbox=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to grayscale has no effect on the bounding box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, color.rgb2gray(img).shape\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting to grayscale removes the third dimension, reducing computational complexity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.HorizontalFlip(p=1.0),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Affine(translate_percent=np.random.random_sample(),p=1.0),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Affine(rotate=np.random.randint(1,359), p=1.0),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.RandomSizedBBoxSafeCrop(350, 350),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy Paste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(img, anns):\n",
    "    mask = np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "    for i in range(len(anns)):\n",
    "        mask = np.maximum(coco.annToMask(anns[i]), mask)\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, anns, img_obj = get_img_and_anns('train')\n",
    "bboxes, cats = get_bboxes_and_cats_from_anns(anns)\n",
    "mask = get_mask(img, anns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_objects(img: np.array, anns: list, coco: COCO):\n",
    "    outputs = []\n",
    "    for ann in anns:\n",
    "        mask = coco.annToMask(ann)\n",
    "        if img.ndim == 3:\n",
    "            img_cropped = img * mask[:,:,np.newaxis]\n",
    "        else:\n",
    "            img_cropped = img * mask[:,:]\n",
    "            img_cropped = img_cropped.reshape(img_cropped.shape[0], img_cropped.shape[1], 1)\n",
    "\n",
    "        rows, cols = np.where(mask)\n",
    "        top_row, bottom_row = rows.min(), rows.max()\n",
    "        left_col, right_col = cols.min(), cols.max()\n",
    "        \n",
    "        if top_row == bottom_row:\n",
    "            bottom_row += 1\n",
    "        \n",
    "        if left_col == right_col:\n",
    "            right_col += 1\n",
    "        \n",
    "        outputs.append((img_cropped[top_row:bottom_row, left_col:right_col], ann))\n",
    "    return outputs\n",
    "\n",
    "coco = COCO(f'{data_dir}/annotations/instances_train2017.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_copies = 2\n",
    "objects = extract_objects(img, anns, coco)\n",
    "img_obj, ann_obj = objects[0]\n",
    "mask_obj = coco.annToMask(ann_obj)\n",
    "\n",
    "pil_img = Image.fromarray(img)\n",
    "\n",
    "for i in range(num_of_copies):\n",
    "    # check if x,y overlap with other objects\n",
    "    mask =np.zeros((img.shape[0], img.shape[1]), dtype=np.uint8)\n",
    "    for ann in anns:\n",
    "        mask = np.maximum(coco.annToMask(ann), mask)\n",
    "        \n",
    "    has_overlap = True\n",
    "    while has_overlap:\n",
    "        # generate random positions\n",
    "        x, y = np.random.randint(\n",
    "            0, img.shape[1]), np.random.randint(0, img.shape[0])\n",
    "\n",
    "        mask_bg_cropped = mask[y:y+img_obj.shape[0], x:x+img_obj.shape[1]]\n",
    "        mask_obj_cropped = cv2.resize(\n",
    "            mask_obj, (mask_bg_cropped.shape[1], mask_bg_cropped.shape[0]))\n",
    "\n",
    "        has_overlap = np.bitwise_and(mask_bg_cropped, mask_obj_cropped).any()\n",
    "        \n",
    "        has_overlap = has_overlap or (\n",
    "            (y+img_obj.shape[0]) > img.shape[0]) or ((x+img_obj.shape[1]) > img.shape[1])\n",
    "    \n",
    "    # blend image\n",
    "    alpha = np.ones(img_obj.shape[:2], dtype=np.float32) * 0.7\n",
    "    alpha = np.dstack((alpha, alpha, alpha))\n",
    "    img_obj_alpha = np.concatenate((img_obj, alpha), axis=2)\n",
    "    \n",
    "    # blend with object\n",
    "    img[y:y+img_obj.shape[0], x:x+img_obj.shape[1],\n",
    "        :] = img[y:y+img_obj.shape[0], x:x+img_obj.shape[1], :] * (1-alpha) + img_obj_alpha[:,:,:3] * alpha\n",
    "    \n",
    "    # img[y:y+img_obj.shape[0], x:x+img_obj.shape[1],:] = img_obj\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Erasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomErasing\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @inproceedings{zhong2020random,\n",
    "# title={Random Erasing Data Augmentation},\n",
    "# author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},\n",
    "# booktitle={Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)},\n",
    "# year={2020}\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.PILToTensor(),\n",
    "    transforms.RandomErasing(p=1, value='random'),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "pil_img = Image.fromarray(img)\n",
    "transformed = np.array(transform(pil_img))\n",
    "\n",
    "visualize(transformed, bboxes,\n",
    "          cats, id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.GaussNoise(p=1),\n",
    "], bbox_params=A.BboxParams(format='coco', label_fields=['category_ids']))\n",
    "\n",
    "transformed = transform(image=img, bboxes=bboxes, category_ids=cats)\n",
    "\n",
    "visualize(transformed['image'], transformed['bboxes'],\n",
    "          transformed['category_ids'], id2label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# save cropped objects\n",
    "def crop_and_save_objects():\n",
    "    if os.path.exists('./data/coco/objects'):\n",
    "        shutil.rmtree('./data/coco/objects')\n",
    "\n",
    "    os.mkdir('./data/coco/objects')\n",
    "\n",
    "    coco = COCO(f'{data_dir}/annotations/instances_train2017.json')\n",
    "    img_ids = load_config()['train_img_ids']\n",
    "    img_objs = coco.loadImgs(img_ids)\n",
    "    for img_obj in img_objs:\n",
    "        img = io.imread(f'{data_dir}/train2017/{img_obj[\"file_name\"]}')\n",
    "        annIds = coco.getAnnIds(imgIds=img_obj['id'], iscrowd=None)\n",
    "        anns = coco.loadAnns(annIds)\n",
    "\n",
    "        objects = extract_objects(img, anns, coco)\n",
    "        \n",
    "        for o in objects:\n",
    "            obj, ann = o\n",
    "            # obj = cv2.resize(obj, (256, 128))\n",
    "            obj_type = id2label[ann[\"category_id\"]]\n",
    "            p = f'./data/coco/objects/{obj_type}'\n",
    "            if not os.path.exists(p):\n",
    "                os.mkdir(p)\n",
    "\n",
    "            cv2.imwrite(\n",
    "                f'{p}/{img_obj[\"id\"]}_{obj_type}.jpg', obj)\n",
    "\n",
    "\n",
    "crop_and_save_objects()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Pretraining_Config():\n",
    "    epochs: int=100\n",
    "    num_patches: int=5\n",
    "    lr_drop: float=40\n",
    "    output_dir: str=\"../output/pretrain\"\n",
    "    batch_size: int=5\n",
    "    obj_path=\"../data/coco/objects\"\n",
    "    lr_backbone:float = 5e-5\n",
    "    resume: str='../output/pretrain/checkpoint.pth'\n",
    "    num_workers: int = 1\n",
    "    coco_path:str='../data/coco'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/dddzg/up-detr.git\n",
    "%cd up-detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import main as pre_main, get_args_parser\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def pretrain_detr(config: Pretraining_Config):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([\n",
    "        '--lr_drop', str(config.lr_drop),\n",
    "        '--num_patches', str(config.num_patches),\n",
    "        '--output_dir', config.output_dir,\n",
    "        '--pre_norm',\n",
    "        '--feature_recon',\n",
    "        '--fre_cnn',\n",
    "        '--imagenet_path', config.obj_path,\n",
    "        '--batch_size', str(config.batch_size),\n",
    "        '--epochs', str(config.epochs),\n",
    "        '--num_workers', str(config.num_workers)\n",
    "    ])\n",
    "\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pre_main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_detr(Pretraining_Config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR Fork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf detr\n",
    "!git clone https://github.com/ahmfrz/detr.git\n",
    "%cd detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Config():\n",
    "    epochs: int=200\n",
    "    obj_types_to_be_selected: int=5\n",
    "    lr: float=1e-4\n",
    "    weight_decay: float=1e-4\n",
    "    output_dir: str=\"..\\output\"\n",
    "    backbone: str=\"resnet50\"\n",
    "    batch_size: int=9\n",
    "    resume: str=\"..\\detr-r50_no-class-head.pth\"\n",
    "    has_custom_detection_class: bool=False\n",
    "    transform_type: str = 'None'\n",
    "    coco_path: str=\"..\\data\\coco\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr.main import main, get_args_parser\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def train_detr(config: Config):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([\n",
    "        '--num_queries', str(config.obj_types_to_be_selected),\n",
    "        '--coco_path', config.coco_path,\n",
    "        '--lr', str(config.lr),\n",
    "        '--weight_decay', str(config.weight_decay),\n",
    "        '--output_dir', config.output_dir,\n",
    "        '--backbone', config.backbone,\n",
    "        '--batch_size', str(config.batch_size),\n",
    "        '--resume', config.resume,\n",
    "        '--epochs', str(config.epochs),\n",
    "        '--has_custom_detection_class', config.has_custom_detection_class,\n",
    "        '--transform_type', config.transform_type\n",
    "    ])\n",
    "\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    main(args)\n",
    "\n",
    "\n",
    "def eval_detr(config: Config):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([\n",
    "        '--coco_path', config.coco_path,\n",
    "        '--batch_size', str(config.batch_size),\n",
    "        '--resume', config.resume,\n",
    "        '--no_aux_loss',\n",
    "        '--eval',\n",
    "    ])\n",
    "\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Get pretrained weights\n",
    "checkpoint = torch.hub.load_state_dict_from_url(\n",
    "    url='https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth',\n",
    "    map_location='cpu',\n",
    "    check_hash=True)\n",
    "\n",
    "# Remove class weights\n",
    "del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "del checkpoint[\"model\"][\"query_embed.weight\"]\n",
    "\n",
    "# Save\n",
    "torch.save(checkpoint,\n",
    "           '../detr-r50_no-class-head.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.plot_utils import plot_logs\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_plots(loc: str):\n",
    "    log_directory = [Path(loc)]\n",
    "    \n",
    "    fields_of_interest = (\n",
    "    'loss',\n",
    "    'mAP',\n",
    "    )\n",
    "\n",
    "    plot_logs(log_directory,\n",
    "            fields_of_interest)\n",
    "    \n",
    "    fields_of_interest = (\n",
    "    'loss_ce',\n",
    "    'loss_bbox',\n",
    "    'loss_giou',\n",
    "    )\n",
    "\n",
    "    plot_logs(log_directory,\n",
    "            fields_of_interest)\n",
    "\n",
    "    fields_of_interest = (\n",
    "        'class_error',\n",
    "        'cardinality_error_unscaled',\n",
    "        )\n",
    "\n",
    "    plot_logs(log_directory,\n",
    "            fields_of_interest)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def load_model(loc: str):\n",
    "    model = torch.hub.load('facebookresearch/detr',\n",
    "                        'detr_resnet50',\n",
    "                        pretrained=False)\n",
    "\n",
    "    checkpoint = torch.load(loc,\n",
    "                            map_location='cpu')\n",
    "\n",
    "    # del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "    # del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "    del checkpoint[\"model\"][\"query_embed.weight\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint['model'],\n",
    "                        strict=False)\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boilerplate functions to display fine-tuned results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors for visualization\n",
    "import torchvision.transforms as T\n",
    "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
    "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
    "\n",
    "\n",
    "# standard PyTorch mean-std input image normalization\n",
    "transform = T.Compose([\n",
    "    T.Resize(800),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# for output bounding box post-processing\n",
    "\n",
    "\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
    "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "def filter_bboxes_from_outputs(outputs,\n",
    "                               size, threshold=0.7):\n",
    "\n",
    "  # keep only predictions with confidence above threshold\n",
    "  probas = outputs['pred_logits'].softmax(-1)[0, :, :-1]\n",
    "  keep = probas.max(-1).values > threshold\n",
    "\n",
    "  probas_to_keep = probas[keep]\n",
    "\n",
    "  # convert boxes from [0; 1] to image scales\n",
    "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'][0, keep], size)\n",
    "\n",
    "  return probas_to_keep, bboxes_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_finetuned_results(pil_img, prob=None, boxes=None):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(pil_img)\n",
    "    ax = plt.gca()\n",
    "    colors = COLORS * 100\n",
    "    if prob is not None and boxes is not None:\n",
    "      for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
    "          ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     fill=False, color=c, linewidth=3))\n",
    "          cl = p.argmax()\n",
    "          text = f'{coco_class_names[cl] if cl in coco_class_names else cl}: {p[cl]:0.2f}'\n",
    "          ax.text(xmin, ymin, text, fontsize=15,\n",
    "                  bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_workflow(my_image, my_model):\n",
    "  # mean-std normalize the input image (batch-size: 1)\n",
    "  img = transform(my_image).unsqueeze(0)\n",
    "\n",
    "  # propagate through the model\n",
    "  outputs = my_model(img)\n",
    "\n",
    "  for threshold in [0.5, 0.4]:\n",
    "\n",
    "    probas_to_keep, bboxes_scaled = filter_bboxes_from_outputs(outputs,my_image.size,\n",
    "                                                               threshold=threshold)\n",
    "\n",
    "    plot_finetuned_results(my_image,\n",
    "                           probas_to_keep,\n",
    "                           bboxes_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a training image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "data_dir = './data/coco/'\n",
    "train_img_ids = load_config()['train_img_ids']\n",
    "coco = COCO(f'{data_dir}/annotations/instances_train2017.json')\n",
    "img_obj = coco.loadImgs(train_img_ids[:5])[4]\n",
    "img_name = f'{data_dir}/train2017/{img_obj[\"file_name\"]}'\n",
    "train_img = Image.open(img_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "val_img_ids = load_config()['val_img_ids']\n",
    "coco = COCO(f'{data_dir}/annotations/instances_val2017.json')\n",
    "img_obj = coco.loadImgs(val_img_ids[:5])[2]\n",
    "img_name = f'{data_dir}/val2017/{img_obj[\"file_name\"]}'\n",
    "val_img = Image.open(img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd detr/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def remove_classification_head(loc: str):\n",
    "    model = torch.hub.load('facebookresearch/detr',\n",
    "                       'detr_resnet50',\n",
    "                       pretrained=False)\n",
    "\n",
    "    checkpoint = torch.load(f'{loc}/checkpoint.pth',\n",
    "                            map_location='cpu')\n",
    "\n",
    "    # del checkpoint[\"model\"][\"class_embed.weight\"]\n",
    "    # del checkpoint[\"model\"][\"class_embed.bias\"]\n",
    "    del checkpoint[\"model\"][\"query_embed.weight\"]\n",
    "\n",
    "    # model.load_state_dict(checkpoint['model'],\n",
    "    #                       strict=False)\n",
    "\n",
    "    torch.save(checkpoint, f'{loc}/classification_head_removed.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config(output_dir='../output/base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_classification_head('../output/base')\n",
    "eval_detr(Config(resume='../output/base/classification_head_removed.pth', coco_path='../data/coco/', batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/base/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/base/checkpoint.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with geometric augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config(output_dir='../output/geometric', \n",
    "                  has_custom_detection_class='True',\n",
    "                  transform_type='geometric'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_classification_head('../output/geometric')\n",
    "eval_detr(Config(resume='../output/geometric/classification_head_removed.pth', coco_path='../data/coco/', batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/geometric')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/geometric/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/geometric/checkpoint.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with Copy Paste augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config(output_dir='../output/copypaste', \n",
    "                  has_custom_detection_class='True',\n",
    "                  transform_type='copypaste',\n",
    "                  batch_size=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_classification_head('../output/copypaste')\n",
    "eval_detr(Config(resume='../output/copypaste/classification_head_removed.pth', coco_path='../data/coco/', batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/copypaste')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/copypaste/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/copypaste/checkpoint.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with Random Erasing augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config(output_dir='../output/randomerasing', \n",
    "                  has_custom_detection_class='True',\n",
    "                  transform_type='randomerasing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_classification_head('../output/randomerasing')\n",
    "eval_detr(Config(resume='../output/randomerasing/classification_head_removed.pth', coco_path='../data/coco/', batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/randomerasing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/randomerasing/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/randomerasing/checkpoint.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with Noise Injection augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config(output_dir='../output/noise', \n",
    "                  has_custom_detection_class='True',\n",
    "                  transform_type='noise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_classification_head('../output/noise')\n",
    "eval_detr(Config(resume='../output/noise/classification_head_removed.pth', coco_path='../data/coco/', batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/noise/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/noise/checkpoint.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with Geometric+Noise Injection augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_detr(Config(output_dir='../output/geometric+noise', \n",
    "                  has_custom_detection_class='True',\n",
    "                  transform_type='geometric+noise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_classification_head('../output/geometric+noise')\n",
    "eval_detr(Config(resume='../output/geometric+noise/classification_head_removed.pth', coco_path='../data/coco/', batch_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/geometric+noise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/geometric+noise/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/geometric+noise/checkpoint.pth'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DETR with Pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd up-detr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detr_main import main, get_args_parser\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "def train_pretrained_detr(config: Pretraining_Config):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'Pretrained DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([\n",
    "        '--lr_drop', str(config.lr_drop),\n",
    "        '--coco_path', config.coco_path,\n",
    "        '--lr_backbone', str(config.lr_backbone),\n",
    "        '--pre_norm',\n",
    "        '--output_dir', config.output_dir,\n",
    "        '--batch_size', str(config.batch_size),\n",
    "        '--pretrain', config.resume,\n",
    "        '--epochs', str(config.epochs),\n",
    "    ])\n",
    "\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    main(args)\n",
    "\n",
    "def eval_pretrained_detr(config: Pretraining_Config):\n",
    "    parser = argparse.ArgumentParser(\n",
    "        'Pretrained DETR training and evaluation script', parents=[get_args_parser()])\n",
    "    args = parser.parse_args([\n",
    "        '--coco_path', config.coco_path,\n",
    "        '--eval',\n",
    "        '--pre_norm',\n",
    "        '--no_aux_loss',\n",
    "        '--batch_size', str(config.batch_size),\n",
    "        '--resume', config.resume,\n",
    "    ])\n",
    "\n",
    "    if args.output_dir:\n",
    "        Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pretrained_detr(Pretraining_Config(output_dir='../output/pretrained_finetuning', \n",
    "                  lr_backbone=5e-5,\n",
    "                  lr_drop=200,\n",
    "                  resume='../output/pretrain/checkpoint.pth',\n",
    "                  epochs=75,\n",
    "                  batch_size=4,\n",
    "                  coco_path='../data/coco'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pretrained_detr(Pretraining_Config(\n",
    "                  resume='../output/pretrained_finetuning/checkpoint.pth',\n",
    "                  batch_size=2,\n",
    "                  coco_path='../data/coco'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_plots('../output/pretrained_finetuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(train_img, load_model('../output/pretrained_finetuning/checkpoint.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_workflow(val_img, load_model('../output/pretrained_finetuning/checkpoint.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "315af1ae600f1a42e83c536ab8e452b8550c968c1ce50e15d6036443c0c9ac51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
